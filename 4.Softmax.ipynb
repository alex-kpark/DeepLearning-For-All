{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Softmax Regression\n",
    "\n",
    "### When do we use Softmax Regression?\n",
    "\n",
    "Multinomial Classification 문제를 해결할 때(n개의 값을 예측할 때), Logistic을 여러번 돌려서 구현할 수도 있지만 번거롭다.\n",
    "\n",
    "Softmax Regression은 Logistic Classification에서의 Weight Function을 합쳐서, 한 번에 계산할 수 있도록 만들어준다.\n",
    "\n",
    "예를 들어, 여러 데이터들이 있을 때 A/B/C로 구분하는 Task가 있다고 하자. 이 때, A or B / A or C / B or C 의 3가지 Logistic을 돌리는 것이 아니라, **A,B,C 각각에 대한 input을 받아서 어떤 것이 가장 Probable한지 확률**로서 접근하는 것이 Softmax Regression이다.\n",
    "\n",
    "A가 될 확률이 0.7 / B가 될 확률이 0.3 / C가 될 확률이 0.1 이런 식으로 나오면 되는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "Cost Function은 기본적으로 예측한 답이 맞으면 낮은 값을 주고, 잘못 예측했을 때 Penalty의 개념으로 1을 주는 것이다.\n",
    "\n",
    "**Cross Entropy**\n",
    "\n",
    "Softmax Regression에서는 Logistic Regression의 Cost Function은 실질적으로 같은 것이다.\n",
    "\n",
    "![soft](img/softcost.png)\n",
    "\n",
    "[A값, B값, C값] = [1,0,0] 이라고 할 때, 1을 전달하면 y값이 무한대로 발산하고 0을 전달하면 0으로 수렴한다. 즉, 잘못 예측했을 때는 1을 전달하여 y를 무한대로 보내 Penalty를 주고 제대로 예측했을 때는 0을 전달해  y를 0으로 보내 맞다는 것을 알려주는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#외부의 값이 들어올 건데(placeholder), float자료형이고 4차원 데이터야!\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "\n",
    "#외부의 값이 들어올 건데(placeholder), float자료형이고 3차원 데이터야!\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "#3개로 분류한다는 뜻이야\n",
    "nb_classes = 3\n",
    "\n",
    "#4(입력데이터) X nb_classes(3개)만큼의 차원에서 Weight을 줄거야\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "\n",
    "#bias도 nb_classes랑 비슷하게 줄거고\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3.3390861)\n",
      "(200, 0.53559661)\n",
      "(400, 0.44353098)\n",
      "(600, 0.37142956)\n",
      "(800, 0.30192119)\n",
      "(1000, 0.2364492)\n",
      "(1200, 0.21347634)\n",
      "(1400, 0.19517291)\n",
      "(1600, 0.17965384)\n",
      "(1800, 0.16633877)\n",
      "(2000, 0.15479784)\n",
      "(array([[  1.19744390e-02,   9.88015234e-01,   1.03607545e-05]], dtype=float32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "#가설설정\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "#크로스 엔트로피\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "    \n",
    "    #arg_max : 몇 번째의 Argument가 가장 큰가요?에 대한 답을 줌\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1,11,7,9]]})\n",
    "    print(a, sess.run(tf.arg_max(a, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
